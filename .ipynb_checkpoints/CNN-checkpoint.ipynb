{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My CNN Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to write down my code for my image classifier using Keras. I think AWS already does a facial recognizer for you, but I wanted to go ahead and write my own code to identify myself from others. I want to see if I can get a regular CNN to work closely to that of a facial recognizer. Lets begin an find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Convolution \n",
    "3 by 3 dimentions. 128 feauture maps will be \n",
    "made. Input_shape() is important because we need to reformat everythig to have\n",
    "shapes. We are working with colored data so we are going to use 3d array.\n",
    "Which is the last number in input_shape() you will see 64, 64 first because\n",
    "this is tensor flow, we normally put something higher like 128 or even 256 \n",
    "because that will leave better results, but becuase we are working with \n",
    "a cpu again we are going to use a smaller amount. We will use relu so we \n",
    "don't have linearity and don't have any negative numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(128,(3,3),input_shape=(128,128,3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Pooling\n",
    "We slide the 2x2 sub table and slide over the feature map and take the max\n",
    "of the four cells. The pooling layer contains all these new maps. We want to \n",
    "reduce our feature maps even more during the pooling layer to decrease our \n",
    "nodes  which decreases the fully connected layer which helps make everything \n",
    "run less computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding a second convolutional layer\n",
    "classifier.add(Convolution2D(128, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding a third convolutional layer \n",
    "classifier.add(Convolution2D(256, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Full Conection \n",
    "\n",
    "Making a classic ANN of layers. Use this input layer made from flattening\n",
    "and then make a hidden layer and an binary output layer because cat or dog. \n",
    "Dense(ouput_dim = amoutn of nodes. We don't know how many to add so lets make\n",
    "a good guess on what to put. 128, is from experimenation. Not too small to \n",
    "decrease accuracy but not too high bec of computation costs. Last parameter\n",
    "lets add the activation function relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Scott/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=200)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 200, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.add(Dropout(.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5 - output later \n",
    "\n",
    "Just copy the layer above but switch to the sigmoid bec binary. Also switch\n",
    "the output_dim to 1 because one thing we want to check, Cats or Dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Scott/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6 - Compiling the CNN\n",
    "\n",
    "Here we chose adam as our optimizer because it is great stochastic optimizer.\n",
    "We chose binary_crossentropy because it is a sigmoid function. However, if it \n",
    "were more than one output, we would probably choose categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "                                                '/Users/Scott/Documents/DSI 6 /Face Recognition/pictures/train',\n",
    "                                                target_size=(128, 128),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "                                            '/Users/Scott/Documents/DSI 6 /Face Recognition/pictures/test',\n",
    "                                            target_size=(128, 128),\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='binary')\n",
    "\n",
    "# classifier.fit_generator(training_set,\n",
    "#                     samples_per_epoch=5000,\n",
    "#                     nb_epoch=25,\n",
    "#                     validation_data=test_set,\n",
    "#                     validation_steps=2000)\n",
    "\n",
    "# I commented it out because this would take awhile on jupyter notebook and it has to be run on AWS for efficiency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
